## Exercises for Module 12 "Infrastructure as Code with Terraform"
<br />

Your K8s cluster on AWS is successfully running and used as a production environment. Your team wants to have additional K8s environments for development, test and staging with the same exact configuration and setup, so they can properly test and try out new features before releasing to production. So you must create 3 more EKS clusters.

But you don't want to do that manually 3 times, so you decide it would be much more efficient to script creating the EKS cluster and execute that same script 3 times to create 3 more identical environments.

<details>
<summary>Exercise 1: Create Terraform project to spin up EKS cluster</summary>
<br />

**Tasks:**

Create a Terraform project that spins up an EKS cluster with the exact same setup that you created in the previous exercise, for the same Java Gradle application:
- Create EKS cluster with 3 Nodes and 1 Fargate profile only for your java application
- Deploy Mysql with 3 replicas with volumes for data persistence using helm

Create a separate git repository for your Terraform project, separate from the Java application, so that changes to the EKS cluster can be made by a separate team independent of the application changes themselves.

**Steps to solve the tasks:**\
**Step 1:** Create an EKS cluster with 3 Nodes and 1 Fargate profile\
Create a `terraform` folder. Within that folder create the following files:

_providers.tf_
```conf
terraform {
  required_version = ">= 1.2.0"
  required_providers {
    aws = {
      source = "hashicorp/aws"
      version = "5.3.0"
    }
  }
}

provider "aws" {
  region  = var.region
}
```

_vpc.tf_
```conf
data "aws_availability_zones" "available" {}

module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "5.0.0"

  name                 = "module12-exercise-vpc"
  cidr                 = "10.0.0.0/16"
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
  public_subnets       = ["10.0.4.0/24", "10.0.5.0/24", "10.0.6.0/24"]
  enable_nat_gateway   = true
  single_nat_gateway   = true
  enable_dns_hostnames = true

  tags = {
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
  }

  public_subnet_tags = {
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
    "kubernetes.io/role/elb"                    = "1"
  }

  private_subnet_tags = {
    "kubernetes.io/cluster/${var.cluster_name}" = "shared"
    "kubernetes.io/role/internal-elb"           = "1"
  }
}
```

_eks-cluster.tf_
```conf
module "eks" {
  source          = "terraform-aws-modules/eks/aws"
  version         = "19.15.3"

  cluster_name    = var.cluster_name
  cluster_version = var.k8s_version
  vpc_id          = module.vpc.vpc_id
  subnet_ids      = module.vpc.private_subnets

  cluster_endpoint_private_access = false
  cluster_endpoint_public_access = true

  tags = {
    Environment = "bootcamp"
    Terraform = "true"
  }

  eks_managed_node_groups = {
    dev = {
      min_size     = 1
      max_size     = 3
      desired_size = 3

      instance_types = ["t3.small"]
      labels = {
        Environment = var.env_prefix
      }
    }
  }

  fargate_profiles = {
    default = {
      name = "module12-exercise-fargate-profile"
      selectors = [
        {
          namespace = "module12-exercise-namespace"
        }
      ]
    }
  }
}
```

_variables.tf_
```conf
variable env_prefix {
  default = "test"
}

variable k8s_version {
  default = "1.27"
}

variable cluster_name {
  default = "module12-exercise-cluster"
}

variable region {
  default = "eu-central-1"
}
```

_dev.tfvars_
```conf
env_prefix = "dev"
k8s_version = "1.27"
cluster_name = "module12-exercise-cluster"
region = "eu-central-1"
```

_output.tf_
```conf
output "cluster_id" {
  description = "EKS cluster ID."
  value       = module.eks.cluster_id
}

output "cluster_endpoint" {
  description = "Endpoint for EKS control plane."
  value       = module.eks.cluster_endpoint
}

output "cluster_security_group_id" {
  description = "Security group ids attached to the cluster control plane."
  value       = module.eks.cluster_security_group_id
}

output "kubectl_config" {
  description = "kubectl config as generated by the module."
  value       = module.eks.aws_auth_configmap_yaml
}

output "config_map_aws_auth" {
  description = "A kubernetes configuration to authenticate to this EKS cluster."
  value       = module.eks.aws_auth_configmap_yaml
}

output "cluster_name" {
  description = "Kubernetes Cluster Name"
  value       = var.cluster_name
}
```

Open the command line, cd into the `terraform` folder and execute the following commands:

```sh
terraform init
terraform apply -var-file="dev.tfvars" --auto-approve
# ...
# Apply complete! Resources: 59 added, 0 changed, 0 destroyed.
# 
# Outputs:
# 
# cluster_endpoint = "https://BC12830B1BC2B87D3979405EBF8DA047.gr7.eu-central-1.eks.amazonaws.com"
# cluster_name = "module12-exercise-cluster"
# cluster_security_group_id = "sg-00cca484b598b71b7"
# config_map_aws_auth = <<EOT
# apiVersion: v1
# kind: ConfigMap
# ...
# EOT
# kubectl_config = <<EOT
# apiVersion: v1
# kind: ConfigMap
# ...
# EOT
```

To access the cluster with `kubectl`, execute:
```sh
aws eks update-kubeconfig --name module12-exercise-cluster --region eu-central-1
# Added new context arn:aws:eks:eu-central-1:369076538622:cluster/module12-exercise-cluster to /Users/fsiegrist/.kube/config

kubectl get nodes
# NAME                                          STATUS   ROLES    AGE    VERSION
# ip-10-0-1-76.eu-central-1.compute.internal    Ready    <none>   4m8s   v1.27.1-eks-2f008fe
# ip-10-0-2-162.eu-central-1.compute.internal   Ready    <none>   4m7s   v1.27.1-eks-2f008fe
# ip-10-0-3-38.eu-central-1.compute.internal    Ready    <none>   4m6s   v1.27.1-eks-2f008fe

eksctl get fargateprofile --cluster module12-exercise-cluster
# NAME                                    SELECTOR_NAMESPACE              SELECTOR_LABELS POD_EXECUTION_ROLE_ARN                                                                     SUBNETS                                                                          TAGS     STATUS
# module12-exercise-fargate-profile       module12-exercise-namespace     <none>          arn:aws:iam::369076538622:role/module12-exercise-fargate-profile-20230617120213663100000001subnet-0aa76a2391180109c,subnet-0344a9d3c6963addb,subnet-0e90ebc6596397679       Environment=bootcamp,Terraform=true       ACTIVE
```

**Step 2:** Deploy Mysql with 3 replicas with volumes for data persistence using helm\
Add the following block defining the 'helm' provider version to the _providers.tf_ file:
```conf
helm = {
  source = "hashicorp/helm"
  version = "2.10.1"
}
```

Add a configuration file for the Mysql deployment to the 'terraform' folder:

_mysql.tf_
```conf
# This gives back object with certificate-authority among other attributes: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/eks_cluster#attributes-reference
data "aws_eks_cluster" "cluster" {
  name = module.eks.cluster_name
}

# This gives us object with token: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/eks_cluster_auth#attributes-reference  
data "aws_eks_cluster_auth" "cluster" {
  name = module.eks.cluster_name
}

provider "helm" {
  kubernetes {
    host = data.aws_eks_cluster.cluster.endpoint
    token = data.aws_eks_cluster_auth.cluster.token
    cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data)
  }
}

resource "helm_release" "mysql" {
  name       = "module12-exercise-release"
  repository = "https://charts.bitnami.com/bitnami"
  chart      = "mysql"
  version    = "9.10.4"
  timeout    = "1000" # seconds (= 16m 40s)

  values = [
    "${file("values.yaml")}"
  ]

  # Set chart values individually
  /* set {
    name  = "volumePermissions.enabled" 
    value = true
  } */
}
```

Apply the changes:

```sh
terraform apply -var-file="dev.tfvars" --auto-approve
# ...
# Plan: 1 to add, 0 to change, 0 to destroy.
# ...
# helm_release.mysql: Creating...
# ...
# helm_release.mysql: Still creating... [16m40s elapsed]
# ╷
# │ Warning: Helm release "module12-exercise-release" was created but has a failed status. Use the `helm` command to investigate the error, correct it, then run Terraform again.
# │ 
# │   with helm_release.mysql,
# │   on mysql.tf line 19, in resource "helm_release" "mysql":
# │   19: resource "helm_release" "mysql" {
# │
# ╵
# ╷
# │ Error: context deadline exceeded
```

Investigate the error:
```sh
helm history module12-exercise-release
# REVISION        UPDATED                         STATUS  CHART           APP VERSION     DESCRIPTION                                                            
# 1               Sat Jun 17 14:46:24 2023        failed  mysql-9.8.2     8.0.33          Release "module12-exercise-release" failed: context deadline exceeded

kubectl get pods
# NAME                                          READY   STATUS    RESTARTS   AGE
# module12-exercise-release-mysql-primary-0     0/1     Pending   0          13m
# module12-exercise-release-mysql-secondary-0   0/1     Pending   0          13m

kubectl logs module12-exercise-release-mysql-primary-0
# Defaulted container "mysql" out of: mysql, volume-permissions (init)

helm uninstall module12-exercise-release
```

</details>

******

<details>
<summary>Exercise 2: Configure remote state</summary>
<br />

**Tasks:**
By default, TF stores state locally. You know that this is not practical when working in a team, because each user must make sure they always have the latest state data before running Terraform. To fix that, you
- configure remote state with a remote data store for your terraform project.

You can use e.g. S3 bucket for storage.

**Steps to solve the tasks:**
Inside the Terraform configuration file `providers.tf` we add a `backend` attribute to the `terraform` block defining the remote AWS S3 state storage:

_terraform/providers.tf_
```conf
terraform {
  ...
  backend "s3" {
    bucket = "module12-exercise-bucket"
    key = "myapp/state.tfstate"
    region = "eu-central-1"
  }
}
```

To use this storage we have to create the bucket on AWS:
- Login to your AWS Management Console and navigate to Services > Storage > S3. The current region automatically switches to "Global".
- Press "Create bucket", enter the bucket name "module12-exercise-bucket" (must be a name which is unique in the global namespace), select your region (eu-central-1), enable Bucket Versioning and leave all the other options unchanged.
- Press "Create bucket".
- Clicking on the newly created bucket opens it, but there are no objects stored yet.

Apply the changes:
```sh
terraform init
# Initializing the backend...
# Do you want to copy existing state to the new backend?
#   Pre-existing state was found while migrating the previous "local" backend to the
#   newly configured "s3" backend. No existing state was found in the newly
#   configured "s3" backend. Do you want to copy this state to the new "s3"
#   backend? Enter "yes" to copy and "no" to start with an empty state.
# 
#   Enter a value: yes
# ...
# Terraform has been successfully initialized!

terraform apply -var-file="dev.tfvars" --auto-approve
# Terraform will perform the following actions:
# 
#   # helm_release.mysql is tainted, so must be replaced
# ...
# Plan: 1 to add, 0 to change, 1 to destroy.
```

</details>

******

<details>
<summary>Exercise 3: CI/CD pipeline for Terraform project</summary>
<br />

Now, the platform team that manages K8s clusters want to make changes to the cluster configurations based on the Infrastructure as Code best practices:

They collaborate and commit changes to git repository and those changes get applied to the cluster through a CI/CD pipeline.

So the AWS infrastructure and K8s cluster changes will be deployed the same way as the application changes, using a CI/CD pipeline.

So the team asks you to help them create a separate Jenkins pipeline for the Terraform project, in addition to your java-app pipeline from the previous module.

**Tasks:**
- Create a separate Jenkins pipeline for Terraform provisioning the EKS cluster

**Steps to solve the tasks:**
Create a Git repository and commit/push the `terraform` folder to it.

Add a 'Jenkinsfile' with the following content to it:

_Jenkinsfile_
```groovy
#!/usr/bin/env groovy

pipeline {
    agent any
    environment {
        AWS_ACCESS_KEY_ID = credentials('jenkins-aws_access_key_id')
        AWS_SECRET_ACCESS_KEY = credentials('jenkins-aws_secret_access_key')
    }
    stages {
        stage('provision cluster') {
            environment {
                TF_VAR_env_prefix = "dev"
                TF_VAR_k8s_version = "1.27"
                TF_VAR_cluster_name = "module12-exercise-cluster"
                TF_VAR_region = "eu-central-1"
            }
            steps {
                script {
                    echo "creating EKS cluster"
                    sh "terraform init"
                    sh "terraform apply --auto-approve"
                    
                    env.EKS_CLUSTER_ENDPOINT = sh(
                        script: "terraform output cluster_endpoint",
                        returnStdout: true
                    ).trim()
                    
                    echo "Cluster URL: ${env.EKS_CLUSTER_ENDPOINT}"
                }
            }
        }
    }
}
```

Create a new pipeline for the project in Jenkins and run it.

### Cleanup
Don't forget to delete the cluster when you're done with the exercise:
```sh
terraform destroy -var-file="dev.tfvars" --auto-approve
```

Also remove the S3 bucket. This has to be done via the AWS Management Console. You first have to empty it before you can delete it.

</details>

******